---
rules:

  - bean: kafka.connect:type=connect-worker-metrics
    prefix: kafka.connect.worker.
    mapping:
      # kafka.connect.worker.connector.count
      connector-count:
        metric: connector.count
        type: updowncounter
        unit: "{connector}"
        desc: The number of connectors run in this worker.
      # kafka.connect.worker.connector.startup.attempts
      connector-startup-attempts-total:
        metric: connector.startup.attempts
        type: counter
        unit: "{attempt}"
        desc: The total number of connector startups that this worker has attempted.
      # kafka.connect.worker.connector.startup.failure.percentage
      connector-startup-failure-percentage:
        metric: connector.startup.failure.percentage
        type: gauge
        unit: '1'
        desc: The average percentage of this worker's connectors starts that failed.
      # kafka.connect.worker.connector.startup.failure.total
      connector-startup-failure-total:
        metric: connector.startup.failure.total
        type: counter
        unit: "{startup}"
        desc: The total number of connector starts that failed.
      # kafka.connect.worker.connector.startup.success.percentage
      connector-startup-success-percentage:
        metric: connector.startup.success.percentage
        type: gauge
        unit: '1'
        desc: The average percentage of this worker's connectors starts that succeeded.
      # kafka.connect.worker.connector.startup.success.total
      connector-startup-success-total:
        metric: connector.startup.success.total
        type: counter
        unit: "{startup}"
        desc: The total number of connector starts that succeeded.
      # kafka.connect.worker.task.count
      task-count:
        metric: task.count
        type: updowncounter
        unit: "{task}"
        desc: The number of tasks run in this worker.
      # kafka.connect.worker.task.startup.attempts
      task-startup-attempts-total:
        metric: task.startup.attempts
        type: counter
        unit: "{attempt}"
        desc: The total number of task startups that this worker has attempted.
      # kafka.connect.worker.task.startup.failure.percentage
      task-startup-failure-percentage:
        metric: task.startup.failure.percentage
        type: gauge
        unit: '1'
        desc: The average percentage of this worker's tasks starts that failed.
      # kafka.connect.worker.task.startup.failure.total
      task-startup-failure-total:
        metric: task.startup.failure.total
        type: counter
        unit: "{startup}"
        desc: The total number of task starts that failed.
      # kafka.connect.worker.task.startup.success.percentage
      task-startup-success-percentage:
        metric: task.startup.success.percentage
        type: gauge
        unit: '1'
        desc: The average percentage of this worker's tasks starts that succeeded.
      # kafka.connect.worker.task.startup.success.total
      task-startup-success-total:
        metric: task.startup.success.total
        type: counter
        unit: "{startup}"
        desc: The total number of task starts that succeeded.

  - bean: kafka.connect:type=connect-worker-metrics,connector=*
    prefix: kafka.connect.worker.connector.task.
    metricAttribute:
      kafka.connect.connector: param(connector)
    type: updowncounter
    unit: "{task}"
    mapping:
      # kafka.connect.worker.connector.task.destroyed
      connector-destroyed-task-count:
        metric: destroyed
        desc: The number of destroyed tasks of the connector on the worker.
      # kafka.connect.worker.connector.task.failed
      connector-failed-task-count:
        metric: failed
        desc: The number of failed tasks of the connector on the worker.
      # kafka.connect.worker.connector.task.paused
      connector-paused-task-count:
        metric: paused
        desc: The number of paused tasks of the connector on the worker.
      # kafka.connect.worker.connector.task.restarting
      connector-restarting-task-count:
        metric: restarting
        desc: The number of restarting tasks of the connector on the worker.
      # kafka.connect.worker.connector.task.running
      connector-running-task-count:
        metric: running
        desc: The number of running tasks of the connector on the worker.
      # kafka.connect.worker.connector.task.total
      connector-total-task-count:
        metric: total
        desc: The number of tasks of the connector on the worker.
      # kafka.connect.worker.connector.task.unassigned
      connector-unassigned-task-count:
        metric: unassigned
        desc: The number of unassigned tasks of the connector on the worker.

  - bean: kafka.connect:type=connect-worker-rebalance-metrics
    prefix: kafka.connect.worker.rebalance.
    mapping:
      # kafka.connect.worker.rebalance.completed.total
      completed-rebalances-total:
        metric: completed.total
        type: counter
        unit: "{rebalance}"
        desc: The total number of rebalances completed by this worker.
      # kafka.connect.worker.rebalance.protocol
      connect-protocol:
        metric: protocol
        type: updowncounter
        desc: The Connect protocol used by this cluster.
        metricAttribute:
          kafka.connect.protocol.state:
            eager: [eager, EAGER]
            cooperative: [compatible, COMPATIBLE, cooperative, COOPERATIVE]
            unknown: "*"
      # kafka.connect.worker.rebalance.epoch
      epoch:
        metric: epoch
        type: updowncounter
        unit: "{epoch}"
        desc: The epoch or generation number of this worker.
      # kafka.connect.worker.rebalance.leader
      # kafka.connect.worker.rebalance.avg.time
      rebalance-avg-time-ms:
        metric: avg.time
        type: gauge
        unit: s
        desc: The average time in milliseconds spent by this worker to rebalance.
      # kafka.connect.worker.rebalance.max.time
      rebalance-max-time-ms:
        metric: max.time
        type: gauge
        unit: s
        desc: The maximum time in milliseconds spent by this worker to rebalance.
      # kafka.connect.worker.rebalance.active
      rebalancing:
        metric: active
        type: updowncounter
        unit: '1'
        desc: Whether this worker is currently rebalancing.
        metricAttribute:
          kafka.connect.worker.rebalance.state:
            rebalancing: ["true", "TRUE"]
            idle: ["false", "FALSE"]
            unknown: "*"
      # kafka.connect.worker.rebalance.since_last
      time-since-last-rebalance-ms:
        metric: since_last
        type: gauge
        unit: s
        desc: The time in milliseconds since this worker completed the most recent rebalance.

  - bean: kafka.connect:type=connector-metrics,connector=*
    prefix: kafka.connect.connector.
    metricAttribute:
      kafka.connect.connector: param(connector)
    mapping:
      # kafka.connect.connector.type
      connector-type:
        metric: type
        type: updowncounter
        desc: The type of the connector. One of 'source' or 'sink'.
        metricAttribute:
          kafka.connect.connector.type:
            sink: [sink, SINK]
            source: [source, SOURCE]
            unknown: "*"
      # kafka.connect.connector.status
      status:
        metric: status
        type: updowncounter
        desc: Connector lifecycle state indicator (1 when the state matches the attribute value). Supports Apache and Confluent status values.
        metricAttribute:
          kafka.connect.connector.state:
            running: [running, RUNNING]
            failed: [failed, FAILED]
            paused: [paused, PAUSED]
            unassigned: [unassigned, UNASSIGNED]
            restarting: [restarting, RESTARTING]
            degraded: [degraded, DEGRADED]
            stopped: [stopped, STOPPED]
            unknown: "*"

  - bean: kafka.connect:type=connector-task-metrics,connector=*,task=*
    prefix: kafka.connect.task.
    metricAttribute:
      kafka.connect.connector: param(connector)
      kafka.connect.task.id: param(task)
    mapping:
      # kafka.connect.task.batch.size.avg
      batch-size-avg:
        metric: batch.size.avg
        type: gauge
        unit: "{record}"
        desc: The average number of records in the batches the task has processed so far.
      # kafka.connect.task.batch.size.max
      batch-size-max:
        metric: batch.size.max
        type: gauge
        unit: "{record}"
        desc: The number of records in the largest batch the task has processed so far.
      # kafka.connect.task.connector.class
      connector-type:
        metric: connector.type
        type: updowncounter
        desc: The type of the connector. One of 'source' or 'sink'.
        metricAttribute:
          kafka.connect.task.connector.type:
            sink: [sink, SINK]
            source: [source, SOURCE]
            unknown: "*"
      # kafka.connect.task.offset.commit.avg.time
      offset-commit-avg-time-ms:
        metric: offset.commit.avg.time
        type: gauge
        unit: s
        desc: The average time in milliseconds taken by this task to commit offsets.
      # kafka.connect.task.offset.commit.failure.percentage
      offset-commit-failure-percentage:
        metric: offset.commit.failure.percentage
        type: gauge
        unit: '1'
        desc: The average percentage of this task's offset commit attempts that failed.
      # kafka.connect.task.offset.commit.max.time
      offset-commit-max-time-ms:
        metric: offset.commit.max.time
        type: gauge
        unit: s
        desc: The maximum time in milliseconds taken by this task to commit offsets.
      # kafka.connect.task.offset.commit.success.percentage
      offset-commit-success-percentage:
        metric: offset.commit.success.percentage
        type: gauge
        unit: '1'
        desc: The average percentage of this task's offset commit attempts that succeeded.
      # kafka.connect.task.pause.ratio
      pause-ratio:
        metric: pause.ratio
        type: gauge
        unit: '1'
        desc: The fraction of time this task has spent in the pause state.
      # kafka.connect.task.running.ratio
      running-ratio:
        metric: running.ratio
        type: gauge
        unit: '1'
        desc: The fraction of time this task has spent in the running state.
      # kafka.connect.task.status
      status:
        metric: status
        type: updowncounter
        desc: The status of the connector task. Supports Apache (unassigned, running, paused, failed, restarting) and Confluent (unassigned, running, paused, failed, destroyed) values.
        metricAttribute:
          kafka.connect.task.state:
            running: [running, RUNNING]
            failed: [failed, FAILED]
            paused: [paused, PAUSED]
            unassigned: [unassigned, UNASSIGNED]
            restarting: [restarting, RESTARTING]
            destroyed: [destroyed, DESTROYED]
            unknown: "*"
      # kafka.connect.task.class

  - bean: kafka.connect:type=sink-task-metrics,connector=*,task=*
    prefix: kafka.connect.sink.
    metricAttribute:
      kafka.connect.connector: param(connector)
      kafka.connect.task.id: param(task)
    mapping:
      # kafka.connect.sink.offset.commit.completion.rate
      offset-commit-completion-rate:
        metric: offset.commit.completion.rate
        type: gauge
        unit: "{commit}/s"
        desc: The average per-second number of offset commit completions that were completed successfully.
      # kafka.connect.sink.offset.commit.completion.total
      offset-commit-completion-total:
        metric: offset.commit.completion.total
        type: counter
        unit: "{commit}"
        desc: The total number of offset commit completions that were completed successfully.
      # kafka.connect.sink.offset.commit.seq
      offset-commit-seq-no:
        metric: offset.commit.seq
        type: updowncounter
        unit: "{sequence}"
        desc: The current sequence number for offset commits.
      # kafka.connect.sink.offset.commit.skip.rate
      offset-commit-skip-rate:
        metric: offset.commit.skip.rate
        type: gauge
        unit: "{commit}/s"
        desc: The average per-second number of offset commit completions that were received too late and skipped/ignored.
      # kafka.connect.sink.offset.commit.skip.total
      offset-commit-skip-total:
        metric: offset.commit.skip.total
        type: counter
        unit: "{commit}"
        desc: The total number of offset commit completions that were received too late and skipped/ignored.
      # kafka.connect.sink.partition.count
      partition-count:
        metric: partition.count
        type: updowncounter
        unit: "{partition}"
        desc: The number of topic partitions assigned to this task belonging to the named sink connector in this worker.
      # kafka.connect.sink.put.batch.avg.time
      put-batch-avg-time-ms:
        metric: put.batch.avg.time
        type: gauge
        unit: s
        desc: The average time taken by this task to put a batch of sinks records.
      # kafka.connect.sink.put.batch.max.time
      put-batch-max-time-ms:
        metric: put.batch.max.time
        type: gauge
        unit: s
        desc: The maximum time taken by this task to put a batch of sinks records.
      # kafka.connect.sink.record.active.count
      sink-record-active-count:
        metric: record.active.count
        type: updowncounter
        unit: "{record}"
        desc: The number of records that have been read from Kafka but not yet completely committed/flushed/acknowledged by the sink task.
      # kafka.connect.sink.record.active.count.avg
      sink-record-active-count-avg:
        metric: record.active.count.avg
        type: gauge
        unit: "{record}"
        desc: The average number of records that have been read from Kafka but not yet completely committed/flushed/acknowledged by the sink task.
      # kafka.connect.sink.record.active.count.max
      sink-record-active-count-max:
        metric: record.active.count.max
        type: gauge
        unit: "{record}"
        desc: The maximum number of records that have been read from Kafka but not yet completely committed/flushed/acknowledged by the sink task.
      # kafka.connect.sink.record.lag.max
      sink-record-lag-max:
        metric: record.lag.max
        type: gauge
        unit: "{record}"
        desc: The maximum lag in terms of number of records that the sink task is behind the consumer's position for any topic partitions.
      # kafka.connect.sink.record.read.rate
      sink-record-read-rate:
        metric: record.read.rate
        type: gauge
        unit: "{record}/s"
        desc: The average per-second number of records read from Kafka for this task belonging to the named sink connector in this worker. This is before transformations are applied.
      # kafka.connect.sink.record.read.total
      sink-record-read-total:
        metric: record.read.total
        type: counter
        unit: "{record}"
        desc: The total number of records read from Kafka by this task belonging to the named sink connector in this worker, since the task was last restarted.
      # kafka.connect.sink.record.send.rate
      sink-record-send-rate:
        metric: record.send.rate
        type: gauge
        unit: "{record}/s"
        desc: The average per-second number of records output from the transformations and sent/put to this task belonging to the named sink connector in this worker. This is after transformations are applied and excludes any records filtered out by the transformations.
      # kafka.connect.sink.record.send.total
      sink-record-send-total:
        metric: record.send.total
        type: counter
        unit: "{record}"
        desc: The total number of records output from the transformations and sent/put to this task belonging to the named sink connector in this worker, since the task was last restarted.

  - bean: kafka.connect:type=source-task-metrics,connector=*,task=*
    prefix: kafka.connect.source.
    metricAttribute:
      kafka.connect.connector: param(connector)
      kafka.connect.task.id: param(task)
    mapping:
      # kafka.connect.source.poll.batch.avg.time
      poll-batch-avg-time-ms:
        metric: poll.batch.avg.time
        type: gauge
        unit: s
        desc: The average time in milliseconds taken by this task to poll for a batch of source records.
      # kafka.connect.source.poll.batch.max.time
      poll-batch-max-time-ms:
        metric: poll.batch.max.time
        type: gauge
        unit: s
        desc: The maximum time in milliseconds taken by this task to poll for a batch of source records.
      # kafka.connect.source.record.active.count
      source-record-active-count:
        metric: record.active.count
        type: updowncounter
        unit: "{record}"
        desc: The number of records that have been produced by this task but not yet completely written to Kafka.
      # kafka.connect.source.record.active.count.avg
      source-record-active-count-avg:
        metric: record.active.count.avg
        type: gauge
        unit: "{record}"
        desc: The average number of records that have been produced by this task but not yet completely written to Kafka.
      # kafka.connect.source.record.active.count.max
      source-record-active-count-max:
        metric: record.active.count.max
        type: gauge
        unit: "{record}"
        desc: The maximum number of records that have been produced by this task but not yet completely written to Kafka.
      # kafka.connect.source.record.poll.rate
      source-record-poll-rate:
        metric: record.poll.rate
        type: gauge
        unit: "{record}/s"
        desc: The average per-second number of records produced/polled (before transformation) by this task belonging to the named source connector in this worker.
      # kafka.connect.source.record.poll.total
      source-record-poll-total:
        metric: record.poll.total
        type: counter
        unit: "{record}"
        desc: The total number of records produced/polled (before transformation) by this task belonging to the named source connector in this worker.
      # kafka.connect.source.record.write.rate
      source-record-write-rate:
        metric: record.write.rate
        type: gauge
        unit: "{record}/s"
        desc: The average per-second number of records written to Kafka for this task belonging to the named source connector in this worker, since the task was last restarted. This is after transformations are applied, and excludes any records filtered out by the transformations.
      # kafka.connect.source.record.write.total
      source-record-write-total:
        metric: record.write.total
        type: counter
        unit: "{record}"
        desc: The number of records output written to Kafka for this task belonging to the named source connector in this worker, since the task was last restarted. This is after transformations are applied, and excludes any records filtered out by the transformations.
      # kafka.connect.source.transaction.size.avg
      transaction-size-avg:
        metric: transaction.size.avg
        type: gauge
        unit: "{record}"
        desc: The average number of records in the transactions the task has committed so far.
      # kafka.connect.source.transaction.size.max
      transaction-size-max:
        metric: transaction.size.max
        type: gauge
        unit: "{record}"
        desc: The number of records in the largest transaction the task has committed so far.
      # kafka.connect.source.transaction.size.min
      transaction-size-min:
        metric: transaction.size.min
        type: gauge
        unit: "{record}"
        desc: The number of records in the smallest transaction the task has committed so far.

  - bean: kafka.connect:type=task-error-metrics,connector=*,task=*
    prefix: kafka.connect.task.error.
    metricAttribute:
      kafka.connect.connector: param(connector)
      kafka.connect.task.id: param(task)
    mapping:
      # kafka.connect.task.error.deadletterqueue.produce.failures
      deadletterqueue-produce-failures:
        metric: deadletterqueue.produce.failures
        type: counter
        unit: "{failure}"
        desc: The number of failed writes to the dead letter queue.
      # kafka.connect.task.error.deadletterqueue.produce.requests
      deadletterqueue-produce-requests:
        metric: deadletterqueue.produce.requests
        type: counter
        unit: "{request}"
        desc: The number of attempted writes to the dead letter queue.
      # kafka.connect.task.error.last.error.timestamp
      last-error-timestamp:
        metric: last.error.timestamp
        type: gauge
        unit: s
        dropNegativeValues: true
        desc: The epoch timestamp when this task last encountered an error.
      # kafka.connect.task.error.total.errors.logged
      total-errors-logged:
        metric: total.errors.logged
        type: counter
        unit: "{error}"
        desc: The number of errors that were logged.
      # kafka.connect.task.error.total.record.errors
      total-record-errors:
        metric: total.record.errors
        type: counter
        unit: "{record}"
        desc: The number of record processing errors in this task.
      # kafka.connect.task.error.total.record.failures
      total-record-failures:
        metric: total.record.failures
        type: counter
        unit: "{record}"
        desc: The number of record processing failures in this task.
      # kafka.connect.task.error.total.records.skipped
      total-records-skipped:
        metric: total.records.skipped
        type: counter
        unit: "{record}"
        desc: The number of records skipped due to errors.
      # kafka.connect.task.error.total.retries
      total-retries:
        metric: total.retries
        type: counter
        unit: "{retry}"
        desc: The number of operations retried.
